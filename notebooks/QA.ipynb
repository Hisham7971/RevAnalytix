{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d296be7-4764-43bd-bf24-8261d50f8a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 22:22:50.454289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b725645b98f4635bbcab565719c6045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6dfe374578474dbfdcc5aba32c7abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14863624a7084d9cab7201fb182858ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a469866a5ec042a6ad8fb557d038d355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7126399d864570b4126daf67d6add3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  which review got the highest score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Marvellous\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  how many positive reviews are there ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = 'data/neg_neu_pos_priority.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Load a pre-trained QA model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# User input loop\n",
    "while True:\n",
    "    user_question = input(\"Enter your question (or type 'exit' to quit): \")\n",
    "    \n",
    "    if user_question.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    # Iterate through the dataset and find the best answer for the user's question\n",
    "    best_answer = None\n",
    "    best_score = float(\"-inf\")\n",
    "    for index, row in df.iterrows():\n",
    "        context = row['content']\n",
    "        question = user_question\n",
    "        answer = qa_pipeline(question=question, context=context)\n",
    "        \n",
    "        # Keep the answer with the highest confidence score\n",
    "        if answer['score'] > best_score:\n",
    "            best_score = answer['score']\n",
    "            best_answer = answer['answer']\n",
    "    \n",
    "    if best_answer:\n",
    "        print(\"Answer:\", best_answer)\n",
    "    else:\n",
    "        print(\"No answer found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ffce7e-4f3e-4313-8b38-76aa7b25d151",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhappytransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HappyTextToText, TTSettings\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandasai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasAI\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandasai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Starcoder\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/happytransformer/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhappytransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhappy_question_answering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HappyQuestionAnswering\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhappytransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhappy_word_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HappyWordPrediction\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhappytransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhappy_text_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HappyTextClassification\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/happytransformer/happy_question_answering.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Union\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForQuestionAnswering, DataCollatorWithPadding, QuestionAnsweringPipeline\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/datasets/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfingerprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_caching, enable_caching, is_caching_enabled, set_caching_enabled\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, MetricInfo\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     get_dataset_config_info,\n\u001b[1;32m     33\u001b[0m     get_dataset_config_names,\n\u001b[1;32m     34\u001b[0m     get_dataset_infos,\n\u001b[1;32m     35\u001b[0m     get_dataset_split_names,\n\u001b[1;32m     36\u001b[0m     inspect_dataset,\n\u001b[1;32m     37\u001b[0m     inspect_metric,\n\u001b[1;32m     38\u001b[0m     list_datasets,\n\u001b[1;32m     39\u001b[0m     list_metrics,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterable_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterableDataset\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_dataset_builder, load_from_disk, load_metric\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/datasets/inspect.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_download_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingDownloadManager\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     dataset_module_factory,\n\u001b[1;32m     33\u001b[0m     get_dataset_builder_class,\n\u001b[1;32m     34\u001b[0m     import_main_class,\n\u001b[1;32m     35\u001b[0m     load_dataset_builder,\n\u001b[1;32m     36\u001b[0m     metric_module_factory,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relative_to_absolute_path\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/datasets/load.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m camelcase_to_snakecase, snakecase_to_camelcase\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaged_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     59\u001b[0m     _EXTENSION_TO_MODULE,\n\u001b[1;32m     60\u001b[0m     _MODULE_SUPPORTS_METADATA,\n\u001b[1;32m     61\u001b[0m     _MODULE_TO_EXTENSIONS,\n\u001b[1;32m     62\u001b[0m     _PACKAGED_DATASETS_MODULES,\n\u001b[1;32m     63\u001b[0m     _hash_python_lines,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Split\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/datasets/packaged_modules/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sha256\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m arrow\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiofolder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audiofolder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csv\n",
      "File \u001b[0;32m~/CodeServer/RevAnalytix/venv/lib/python3.11/site-packages/datasets/packaged_modules/arrow/arrow.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table_cast\n\u001b[0;32m---> 11\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArrowConfig\u001b[39;00m(datasets\u001b[38;5;241m.\u001b[39mBuilderConfig):\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BuilderConfig for Arrow.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'datasets' has no attribute 'utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from happytransformer import HappyTextToText, TTSettings\n",
    "from pandasai import PandasAI\n",
    "from pandasai.llm import Starcoder\n",
    "\n",
    "# Define a function to process questions and generate responses\n",
    "def process_question(user_input, reference_questions, data_path):\n",
    "    try:\n",
    "        happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "        args = TTSettings(num_beams=5, min_length=1)\n",
    "        result = happy_tt.generate_text(\"grammar: \" + user_input, args=args)\n",
    "        corrected_user_input = result.text\n",
    "\n",
    "        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "        user_input_embedding = model.encode(corrected_user_input, convert_to_tensor=True)\n",
    "        reference_question_embeddings = model.encode(reference_questions, convert_to_tensor=True)\n",
    "        similarity_scores = util.pytorch_cos_sim(user_input_embedding, reference_question_embeddings)[0]\n",
    "\n",
    "        for i, score in enumerate(similarity_scores):\n",
    "            if score > 0.4:\n",
    "                try:\n",
    "                    data = pd.read_csv(data_path)  # Load your table data from a CSV file\n",
    "\n",
    "                    if data is None or len(data) == 0:\n",
    "                        return \"Upload a data file to proceed.\"\n",
    "\n",
    "                    API_key = os.environ.get('API_key')\n",
    "                    llm = Starcoder(api_token=API_key)\n",
    "                    pandas_ai = PandasAI(llm, conversational=False, verbose=True)\n",
    "\n",
    "                    response = pandas_ai.run(data, prompt=corrected_user_input)\n",
    "\n",
    "                    if isinstance(response, int) or isinstance(response, float):\n",
    "                        return str(response)\n",
    "                    elif isinstance(response, str):\n",
    "                        return response\n",
    "                    elif isinstance(response, pd.DataFrame):\n",
    "                        return response.to_html(classes='table table-bordered', index=False)\n",
    "                    elif isinstance(response, pd.Series):\n",
    "                        response_df = pd.DataFrame({'': response.index, 'Count': response.values})\n",
    "                        return response_df.to_html(classes='table table-bordered', index=False)\n",
    "                    else:\n",
    "                        return str(response)\n",
    "\n",
    "                except Exception as e:\n",
    "                    return f\"An error occurred while processing the question: {str(e)}\"\n",
    "\n",
    "        return \"Your question doesn't meet the similarity threshold.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Example usage:\n",
    "user_input = \"What is the total revenue?\"\n",
    "reference_questions = [\"What is the total revenue?\", \"How many items were sold?\", \"What is the average price?\"]\n",
    "data_path = \"your_data.csv\"  # Specify the path to your CSV file containing the table data\n",
    "response = process_question(user_input, reference_questions, data_path)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f5fde-2270-4de2-bb88-95b97003dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bert-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a5d8a-a3c7-49e0-875d-664cbe426882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eaaea8-4215-49d0-babd-47937105fa9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70ddffe5-873e-40df-b659-4268fc1262af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I was not able to answer your question, because of the following error:\n",
      "\n",
      "You exceeded your current quota, please check your plan and billing details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm import OpenAI\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('data/neg_neu_pos_priority.csv')\n",
    "\n",
    "# Instantiate a LLM\n",
    "llm = OpenAI(api_token=\"YOUR_API_KEY\")\n",
    "\n",
    "# Create a SmartDataframe\n",
    "df = SmartDataframe(df, config={\"llm\": llm})\n",
    "\n",
    "# Ask a question\n",
    "print(df.chat('Which are the 5 happiest countries?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52389c4-f9d5-48f5-acb7-a33a9cd21490",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
