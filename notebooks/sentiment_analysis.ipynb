{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1461c5b-390c-456a-84fd-557865b56d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import emoji\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7eab93-b109-4376-a2ce-0cf77287efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.9622\n",
      "2) neutral 0.0275\n",
      "3) negative 0.0103\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    # Convert emojis to their text representation\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return ' '.join(text.split())\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"I've been using Epic! with my now 4 year old for over a year. It's awesome. There are a plethora of great books. They miss many of my absolute favorites, but the ones they have are very good and we've found new favorites. And tons of audiobooks! Those are fantastic to have. However, finding all the books in a reading level is impossible, and I very much wish that was MUCH easier. I create lists myself and have found workarounds, but it's annoying and I wish we could search by reading level only.\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0311ab-af75-4242-97f7-c90a79877121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999782e7-e0ca-4875-9d91-17b2e3a67a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function\n",
    "def preprocess(text):\n",
    "    # Convert emojis to their text representation\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('data/in.evolve.android.csv')  # Replace 'your_data.csv' with the actual file path\n",
    "\n",
    "# Initialize sentiment analysis model and tokenizer\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Lists to store sentiment and sentiment scores\n",
    "sentiments = []\n",
    "sentiment_scores = []\n",
    "\n",
    "# Chunk size for processing long texts\n",
    "chunk_size = 256  # Adjust this value as needed\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    text = preprocess(row['content'])  # Preprocess the 'content' column\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    chunk_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if chunk:\n",
    "            encoded_input = tokenizer(\n",
    "                chunk,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            output = model(**encoded_input)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            ranking = np.argsort(scores)\n",
    "            ranking = ranking[::-1]\n",
    "        \n",
    "            # Get the top sentiment label and score for the chunk\n",
    "            top_label = config.id2label[ranking[0]]\n",
    "            top_score = np.round(float(scores[ranking[0]]), 4)\n",
    "        \n",
    "            chunk_scores.append(top_score)\n",
    "    \n",
    "    # Calculate the average score for all chunks if there are scores\n",
    "    if chunk_scores:\n",
    "        average_score = np.mean(chunk_scores)\n",
    "    else:\n",
    "        # Handle the case where there are no scores (empty text)\n",
    "        average_score = np.nan\n",
    "    \n",
    "    # Append the results to the lists\n",
    "    sentiments.append(top_label)\n",
    "    sentiment_scores.append(average_score)\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "df['sentiment'] = sentiments\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "# Save the DataFrame with the new columns\n",
    "df.to_csv('data/neg_neu_pos.csv', index=False)  # Replace 'output_data.csv' with the desired output file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eac439-3650-4ba5-a849-9433f812ed86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
