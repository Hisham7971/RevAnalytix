{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae881a3f-40a3-4022-b899-5813b8870bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d44da3-de65-45e8-abe2-ac5c581dc57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Preprocess function\n",
    "def preprocess(text):\n",
    "    # Convert emojis to their text representation\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Load the CSV data\n",
    "df = pd.read_csv('data/in.evolve.android.csv')\n",
    "\n",
    "# Initialize sentiment analysis model and tokenizer\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Lists to store sentiment and sentiment scores\n",
    "sentiments = []\n",
    "sentiment_scores = []\n",
    "\n",
    "# Chunk size for processing long texts\n",
    "chunk_size = 256\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    text = preprocess(row['content'])\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    chunk_scores = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if chunk:\n",
    "            encoded_input = tokenizer(\n",
    "                chunk,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            output = model(**encoded_input)\n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            ranking = np.argsort(scores)\n",
    "            ranking = ranking[::-1]\n",
    "        \n",
    "            # Get the top sentiment label and score for the chunk\n",
    "            top_label = config.id2label[ranking[0]]\n",
    "            top_score = np.round(float(scores[ranking[0]]), 4)\n",
    "        \n",
    "            chunk_scores.append(top_score)\n",
    "    \n",
    "    # Calculate the average score for all chunks if there are scores\n",
    "    if chunk_scores:\n",
    "        average_score = np.mean(chunk_scores)\n",
    "    else:\n",
    "        average_score = np.nan\n",
    "    \n",
    "    # Append the results to the lists\n",
    "    sentiments.append(top_label)\n",
    "    sentiment_scores.append(average_score)\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "df['sentiment'] = sentiments\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "# Define priority levels based on sentiment scores\n",
    "def categorize_priority(score):\n",
    "    if score >= 0.75:\n",
    "        return 'P1'  # most negative reviews\n",
    "    elif score >= 0.50:\n",
    "        return 'P2'  # moderately negative reviews\n",
    "    else:\n",
    "        return 'P3'  # least negative reviews\n",
    "\n",
    "# Apply the function to the 'sentiment_score' column for negative reviews\n",
    "df.loc[df['sentiment'] == 'negative', 'priority'] = df.loc[df['sentiment'] == 'negative', 'sentiment_score'].apply(categorize_priority)\n",
    "\n",
    "# Save the DataFrame with the new columns\n",
    "df.to_csv('data/neg_neu_pos_priority.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d135168-3f2e-49af-9205-200cfb1acaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
